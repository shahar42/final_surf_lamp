Data Layer Complete Specification
===================================

DOMAIN RESPONSIBILITY: Database operations, caching, data persistence, transaction management
FORBIDDEN: Business logic, HTTP handling, external API calls, background scheduling

INTERFACE CONTRACTS TO IMPLEMENT:
=================================

YOU MUST IMPLEMENT these interfaces:

1. ILampRepository - Lamp configuration and status management
2. IUserRepository - User account management
3. IActivityLogger - Activity and audit logging
4. ICacheManager - Surf data caching with Redis

REQUIRED DIRECTORY STRUCTURE:
============================

Create these files:
```
app/data_layer/
├── repositories/
│   ├── lamp_repository.py        # ILampRepository implementation
│   ├── user_repository.py        # IUserRepository implementation
│   └── activity_logger.py        # IActivityLogger implementation
├── cache/
│   └── cache_manager.py          # ICacheManager implementation
├── models/
│   ├── database_models.py        # SQLAlchemy ORM models
│   └── data_types.py             # LampConfig, SurfData implementations
├── migrations/
│   ├── alembic.ini
│   ├── env.py
│   └── versions/
│       └── 001_initial_schema.py
└── connection/
    ├── database_manager.py       # Connection pool management
    └── redis_manager.py          # Redis connection management
```

POSTGRESQL DATABASE SCHEMA:
===========================

Complete Database Schema (SQL):

```sql
-- Lamp Registry Table
CREATE TABLE lamp_registry (
    lamp_id UUID PRIMARY KEY,
    email VARCHAR(255) NOT NULL,
    name VARCHAR(255) NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    location_index INTEGER NOT NULL,
    brightness INTEGER DEFAULT 100,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_active BOOLEAN DEFAULT true,
    last_seen TIMESTAMP,
    firmware_version VARCHAR(50)
);

-- API Configuration Table
CREATE TABLE api_configuration (
    id SERIAL PRIMARY KEY,
    website_name VARCHAR(100) NOT NULL,
    full_url VARCHAR(500) NOT NULL,
    is_json BOOLEAN NOT NULL DEFAULT true,
    is_metric BOOLEAN NOT NULL DEFAULT true,
    api_key VARCHAR(255),
    city JSONB NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_active BOOLEAN DEFAULT true,
    priority_order INTEGER DEFAULT 1
);

-- System Configuration Table
CREATE TABLE system_configuration (
    id SERIAL PRIMARY KEY,
    config_key VARCHAR(100) NOT NULL UNIQUE,
    config_value JSONB NOT NULL,
    description TEXT,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_active BOOLEAN DEFAULT true
);

-- Activity Log Table
CREATE TABLE activity_log (
    id BIGSERIAL PRIMARY KEY,
    lamp_id UUID,
    activity_type VARCHAR(50) NOT NULL,
    status VARCHAR(20) NOT NULL,
    details JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    ip_address INET,
    user_agent TEXT,
    FOREIGN KEY (lamp_id) REFERENCES lamp_registry(lamp_id) ON DELETE SET NULL
);

-- User Sessions Table (for future web interface)
CREATE TABLE user_sessions (
    id UUID PRIMARY KEY,
    lamp_id UUID NOT NULL,
    session_token VARCHAR(255) NOT NULL UNIQUE,
    expires_at TIMESTAMP NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (lamp_id) REFERENCES lamp_registry(lamp_id) ON DELETE CASCADE
);

-- Performance Indexes
CREATE INDEX idx_lamp_registry_email ON lamp_registry(email);
CREATE INDEX idx_lamp_registry_location ON lamp_registry(location_index);
CREATE INDEX idx_lamp_registry_active ON lamp_registry(is_active);
CREATE INDEX idx_lamp_registry_last_seen ON lamp_registry(last_seen);
CREATE INDEX idx_api_config_active_priority ON api_configuration(is_active, priority_order);
CREATE INDEX idx_activity_log_lamp_time ON activity_log(lamp_id, created_at);
CREATE INDEX idx_activity_log_type_status ON activity_log(activity_type, status);
CREATE INDEX idx_activity_log_created_at ON activity_log(created_at);
CREATE INDEX idx_system_config_key ON system_configuration(config_key);

-- Initial Data Setup
INSERT INTO system_configuration (config_key, config_value, description) VALUES
('supported_locations', 
 '[
   {"index": 0, "name": "San Diego", "state": "CA", "coordinates": {"lat": 32.7157, "lon": -117.1611}},
   {"index": 1, "name": "Santa Cruz", "state": "CA", "coordinates": {"lat": 36.9741, "lon": -122.0308}},
   {"index": 2, "name": "Honolulu", "state": "HI", "coordinates": {"lat": 21.3099, "lon": -157.8581}},
   {"index": 3, "name": "Huntington Beach", "state": "CA", "coordinates": {"lat": 33.6595, "lon": -117.9988}},
   {"index": 4, "name": "Malibu", "state": "CA", "coordinates": {"lat": 34.0259, "lon": -118.7798}}
 ]',
 'Supported coastal cities with location indexes and coordinates'),
('cache_settings',
 '{"surf_data_ttl": 1800, "location_cache_ttl": 3600, "max_cache_size": 1000}',
 'Cache configuration settings'),
('api_settings',
 '{"request_timeout": 30, "max_retries": 3, "retry_delay": 5}',
 'External API configuration settings');

-- Sample API Configuration Data
INSERT INTO api_configuration (website_name, full_url, is_json, is_metric, api_key, city, priority_order) VALUES
('Surfline', 'https://services.surfline.com/kbyg/spots/forecasts', true, false, NULL, 
 '{"name": "San Diego", "spot_id": "5842041f4e65fad6a7708876"}', 1),
('WeatherAPI', 'https://api.weatherapi.com/v1/marine.json', true, true, 'WEATHER_API_KEY_PLACEHOLDER',
 '{"name": "San Diego", "q": "32.7157,-117.1611"}', 2);
```

CRITICAL REPOSITORY IMPLEMENTATIONS:
====================================

1. LAMP REPOSITORY (ILampRepository)
   ----------------------------------

   Class: PostgresLampRepository(ILampRepository)

   Method: get_lamp_configuration(lamp_id: str) -> Optional[LampConfig]
   
   Implementation Requirements:
   ```sql
   -- Complex query joining lamp with API configurations
   SELECT 
       lr.lamp_id,
       lr.email,
       lr.name,
       lr.location_index,
       lr.brightness,
       lr.is_active,
       lr.last_seen,
       sc.config_value as location_data,
       COALESCE(
           json_agg(
               json_build_object(
                   'website_name', ac.website_name,
                   'full_url', ac.full_url,
                   'api_key', ac.api_key,
                   'city', ac.city,
                   'is_json', ac.is_json,
                   'is_metric', ac.is_metric,
                   'priority', ac.priority_order
               )
               ORDER BY ac.priority_order
           ) FILTER (WHERE ac.id IS NOT NULL), 
           '[]'::json
       ) as api_configs
   FROM lamp_registry lr
   LEFT JOIN api_configuration ac ON ac.is_active = true
   LEFT JOIN system_configuration sc ON sc.config_key = 'supported_locations'
   WHERE lr.lamp_id = $1 AND lr.is_active = true
   GROUP BY lr.lamp_id, lr.email, lr.name, lr.location_index, 
            lr.brightness, lr.is_active, lr.last_seen, sc.config_value;
   ```

   Return Format:
   ```python
   LampConfig({
       "lamp_id": "uuid-string",
       "location_index": 2,
       "location_name": "Honolulu", 
       "brightness": 100,
       "is_active": True,
       "last_seen": "2025-01-01T10:00:00Z",
       "api_configs": [
           {
               "website_name": "Surfline",
               "full_url": "https://...",
               "api_key": "key123",
               "city": {"name": "Honolulu", "spot_id": "..."},
               "priority": 1
           }
       ]
   })
   ```

   Method: register_new_lamp(lamp_data: Dict[str, Any]) -> bool
   
   Transaction Requirements:
   ```python
   async def register_new_lamp(self, lamp_data: Dict[str, Any]) -> bool:
       async with self.db_pool.acquire() as conn:
           async with conn.transaction():
               try:
                   # Insert lamp record
                   await conn.execute(
                       """INSERT INTO lamp_registry 
                          (lamp_id, email, name, password_hash, location_index, brightness)
                          VALUES ($1, $2, $3, $4, $5, $6)""",
                       lamp_data["lamp_id"],
                       lamp_data["email"], 
                       lamp_data["name"],
                       lamp_data["password_hash"],
                       lamp_data["location_index"],
                       lamp_data.get("brightness", 100)
                   )
                   
                   # Log registration activity
                   await conn.execute(
                       """INSERT INTO activity_log 
                          (lamp_id, activity_type, status, details)
                          VALUES ($1, 'registration', 'success', $2)""",
                       lamp_data["lamp_id"],
                       json.dumps({"email": lamp_data["email"], "location": lamp_data["location_index"]})
                   )
                   
                   return True
               except Exception as e:
                   logger.error("Lamp registration failed", error=str(e), lamp_data=lamp_data)
                   raise DatabaseError(f"Registration failed: {e}") from e
   ```

   Method: get_all_active_lamps() -> List[LampConfig]
   
   Purpose: Get all active lamps for background processing
   
   Implementation:
   ```sql
   SELECT DISTINCT 
       lamp_id, 
       location_index,
       brightness,
       last_seen
   FROM lamp_registry 
   WHERE is_active = true
   ORDER BY last_seen DESC NULLS LAST;
   ```

   Method: update_lamp_status(lamp_id: str, status: str) -> bool
   
   Implementation:
   ```sql
   UPDATE lamp_registry 
   SET is_active = $2, 
       last_seen = CURRENT_TIMESTAMP,
       updated_at = CURRENT_TIMESTAMP
   WHERE lamp_id = $1;
   ```

2. USER REPOSITORY (IUserRepository)
   ----------------------------------

   Class: PostgresUserRepository(IUserRepository)

   Method: register_user(user_data: Dict[str, Any]) -> bool
   
   Implementation:
   ```python
   async def register_user(self, user_data: Dict[str, Any]) -> bool:
       async with self.db_pool.acquire() as conn:
           try:
               # Check for existing user
               existing = await conn.fetchval(
                   "SELECT lamp_id FROM lamp_registry WHERE email = $1",
                   user_data["email"]
               )
               
               if existing:
                   raise ValidationError("User already registered")
               
               # Insert new user (lamp_registry serves as user table)
               await conn.execute(
                   """INSERT INTO lamp_registry 
                      (lamp_id, email, name, password_hash, location_index)
                      VALUES ($1, $2, $3, $4, $5)""",
                   user_data["lamp_id"],
                   user_data["email"],
                   user_data["name"], 
                   user_data["password_hash"],
                   user_data["location_index"]
               )
               
               return True
           except ValidationError:
               raise
           except Exception as e:
               logger.error("User registration failed", error=str(e))
               raise DatabaseError(f"User registration failed: {e}") from e
   ```

   Method: validate_user_credentials(email: str, password_hash: str) -> bool
   
   Implementation:
   ```sql
   SELECT lamp_id 
   FROM lamp_registry 
   WHERE email = $1 AND password_hash = $2 AND is_active = true;
   ```

3. ACTIVITY LOGGER (IActivityLogger)
   ----------------------------------

   Class: PostgresActivityLogger(IActivityLogger)

   Method: log_activity(lamp_id: str, activity_type: str, status: str, details: Optional[Dict[str, Any]]) -> None

   Implementation:
   ```python
   async def log_activity(self, lamp_id: str, activity_type: str, 
                         status: str, details: Optional[Dict[str, Any]] = None) -> None:
       async with self.db_pool.acquire() as conn:
           try:
               await conn.execute(
                   """INSERT INTO activity_log 
                      (lamp_id, activity_type, status, details, created_at)
                      VALUES ($1, $2, $3, $4, CURRENT_TIMESTAMP)""",
                   lamp_id,
                   activity_type,
                   status,
                   json.dumps(details) if details else None
               )
           except Exception as e:
               # Don't let logging errors break main functionality
               logger.error("Activity logging failed", 
                          lamp_id=lamp_id, 
                          activity_type=activity_type,
                          error=str(e))
   ```

   Additional Methods:
   ```python
   async def get_lamp_activity_history(self, lamp_id: str, limit: int = 100) -> List[Dict]:
       """Get recent activity for a lamp"""
       
   async def get_activity_statistics(self, hours: int = 24) -> Dict[str, Any]:
       """Get system-wide activity statistics"""
   ```

REDIS CACHE MANAGER:
===================

Class: RedisCacheManager(ICacheManager)

Redis Key Strategy:
- surf_data:{location_index} - Cached surf data
- location_meta:{location_index} - Location metadata
- system_stats - System statistics
- lamp_status:{lamp_id} - Lamp status cache

Method: get_surf_data_cache(location_index: int) -> Optional[SurfData]

Implementation:
```python
async def get_surf_data_cache(self, location_index: int) -> Optional[SurfData]:
    cache_key = f"surf_data:{location_index}"
    try:
        cached_json = await self.redis.get(cache_key)
        if cached_json:
            data = json.loads(cached_json)
            
            # Check if data is still fresh
            cache_timestamp = data.get("cache_timestamp")
            if cache_timestamp:
                cache_age = time.time() - cache_timestamp
                if cache_age > 1800:  # 30 minutes
                    logger.info("Cache expired", location_index=location_index, age=cache_age)
                    await self.redis.delete(cache_key)
                    return None
            
            logger.info("Cache hit", location_index=location_index)
            return SurfData(data)
        
        logger.info("Cache miss", location_index=location_index)
        return None
        
    except Exception as e:
        logger.error("Cache read error", location_index=location_index, error=str(e))
        return None
```

Method: set_surf_data_cache(location_index: int, data: SurfData, ttl_seconds: int = 1800) -> None

Implementation:
```python
async def set_surf_data_cache(self, location_index: int, data: SurfData, ttl_seconds: int = 1800) -> None:
    cache_key = f"surf_data:{location_index}"
    try:
        # Add cache metadata
        cache_data = dict(data)
        cache_data["cache_timestamp"] = time.time()
        cache_data["ttl_seconds"] = ttl_seconds
        
        # Serialize and store
        cache_json = json.dumps(cache_data, default=str)
        await self.redis.setex(cache_key, ttl_seconds, cache_json)
        
        logger.info("Cache updated", 
                   location_index=location_index, 
                   ttl=ttl_seconds,
                   data_size=len(cache_json))
        
    except Exception as e:
        logger.error("Cache write error", 
                    location_index=location_index, 
                    error=str(e))
```

Method: invalidate_cache(location_index: int) -> None

Implementation:
```python
async def invalidate_cache(self, location_index: int) -> None:
    cache_key = f"surf_data:{location_index}"
    try:
        deleted = await self.redis.delete(cache_key)
        logger.info("Cache invalidated", 
                   location_index=location_index, 
                   was_present=bool(deleted))
    except Exception as e:
        logger.error("Cache invalidation error", 
                    location_index=location_index, 
                    error=str(e))
```

CONNECTION MANAGEMENT:
=====================

1. DATABASE CONNECTION MANAGER
   ----------------------------

   Class: DatabaseManager

   Connection Pool Configuration:
   ```python
   async def create_pool(self) -> asyncpg.Pool:
       return await asyncpg.create_pool(
           self.database_url,
           min_size=5,
           max_size=20,
           max_queries=50000,
           max_inactive_connection_lifetime=300,
           timeout=30,
           command_timeout=60,
           server_settings={
               'jit': 'off',  # Disable JIT for better predictability
               'application_name': 'surfboard_lamp_backend'
           }
       )
   ```

   Health Monitoring:
   ```python
   async def get_pool_status(self) -> Dict[str, Any]:
       return {
           "size": self.pool.get_size(),
           "min_size": self.pool.get_min_size(),
           "max_size": self.pool.get_max_size(),
           "idle_size": self.pool.get_idle_size(),
           "queries_count": self.pool.get_queries_count()
       }
   ```

2. REDIS CONNECTION MANAGER
   -------------------------

   Class: RedisManager

   Connection Configuration:
   ```python
   async def create_redis(self) -> redis.asyncio.Redis:
       return redis.asyncio.Redis(
           host=self.redis_host,
           port=self.redis_port,
           db=0,
           password=self.redis_password,
           encoding='utf-8',
           decode_responses=True,
           socket_timeout=30,
           socket_connect_timeout=10,
           health_check_interval=30,
           max_connections=20
       )
   ```

DATA MODEL IMPLEMENTATIONS:
==========================

1. LAMP CONFIG DATA TYPE
   ----------------------

   ```python
   class LampConfig(Dict[str, Any]):
       def __init__(self, data: Dict[str, Any]):
           super().__init__(data)
           self._validate_required_fields()
           self._enrich_location_data()
       
       def _validate_required_fields(self):
           required = ["lamp_id", "location_index", "brightness"]
           for field in required:
               if field not in self:
                   raise ValueError(f"Missing required field: {field}")
       
       def _enrich_location_data(self):
           """Add location name and coordinates from system config"""
           location_index = self["location_index"]
           # Look up location data and add to config
           
       @property
       def location_name(self) -> str:
           return self.get("location_name", f"Location {self['location_index']}")
       
       @property
       def is_active(self) -> bool:
           return self.get("is_active", False)
   ```

2. SURF DATA TYPE
   ---------------

   ```python
   class SurfData(Dict[str, Any]):
       def __init__(self, data: Optional[Dict[str, Any]] = None):
           base_structure = {
               "wave_height_m": None,
               "wave_period_s": None,
               "wind_speed_mps": None,
               "wind_deg": None,
               "location_name": "",
               "timestamp": None,
               "data_source": "",
               "cache_timestamp": None
           }
           base_structure.update(data or {})
           super().__init__(base_structure)
       
       @property
       def is_complete(self) -> bool:
           """Check if all required fields are present"""
           required = ["wave_height_m", "wave_period_s", "wind_speed_mps"]
           return all(self.get(field) is not None for field in required)
       
       @property
       def age_minutes(self) -> Optional[float]:
           """Get data age in minutes"""
           if self.get("timestamp"):
               return (time.time() - self["timestamp"]) / 60
           return None
   ```

PERFORMANCE OPTIMIZATION:
========================

Query Optimization Strategies:
1. Use connection pooling with appropriate size
2. Implement query result caching for static data
3. Use database indexes for all frequent queries
4. Batch operations where possible
5. Monitor query performance with logging

Cache Optimization:
1. Use appropriate TTL values (30 min for surf data)
2. Implement cache warming for popular locations
3. Use Redis pipelining for bulk operations
4. Monitor cache hit rates and adjust strategies

Database Monitoring:
```python
async def log_query_performance(operation: str, duration_ms: float, rows_affected: int):
    logger.info("Database operation", 
               operation=operation,
               duration_ms=duration_ms,
               rows_affected=rows_affected)
    
    # Alert on slow queries
    if duration_ms > 1000:  # > 1 second
        logger.warning("Slow query detected", 
                      operation=operation,
                      duration_ms=duration_ms)
```

CRITICAL SUCCESS CRITERIA:
=========================

The data layer implementation MUST:
✅ Implement all repository and cache interfaces completely
✅ Use asyncpg connection pooling for optimal PostgreSQL performance
✅ Implement Redis caching with proper TTL and error handling
✅ Handle all database transactions atomically
✅ Include comprehensive error handling and logging
✅ Support concurrent operations safely
✅ Never expose database implementation details to other layers
✅ Maintain data integrity with proper constraints and validations
✅ Scale efficiently with connection pooling and caching
✅ Provide monitoring capabilities for performance tuning
✅ Support lamp configuration retrieval in < 100ms
✅ Cache surf data efficiently with 30-minute TTL

Generate complete, production-ready data layer implementation following these specifications.
